{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/billdjomkam/Bureau/Test_extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(org,excel_file,metaDataset):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "    #from google.colab import files\n",
    "    #import model\n",
    "    import tensorflow as tf\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import math\n",
    "    from keras.utils import np_utils\n",
    "    from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam, Nadam\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from sklearn import metrics\n",
    "    import json\n",
    "    #\n",
    "    from sklearn.model_selection import train_test_split \n",
    "    import seaborn as sns\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import style\n",
    "    #%matplotlib inline\n",
    "    # Evaluations\n",
    "    from sklearn.metrics import classification_report,accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, f1_score\n",
    "    # Random Forest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    #######################################DEFINE FUNCTION #################################################################\n",
    "    #def function \n",
    "\n",
    "    def getScaledData(dataMatrix):\n",
    "        scaler = StandardScaler().fit(dataMatrix)\n",
    "        return scaler.transform(dataMatrix)\n",
    "\n",
    "    # separating feature matrix and class label\n",
    "    def separateDataAndClassLabel(dataMatrix):\n",
    "        featureMatrix = dataMatrix[:, :(dataMatrix.shape[1] - 1)]\n",
    "        classLabelMatrix = dataMatrix[:, -1]\n",
    "\n",
    "        return featureMatrix, classLabelMatrix\n",
    "\n",
    "\n",
    "    # returns the number of classes and encode it\n",
    "    def encodeClassLabel(classLabel):\n",
    "        labelEncoder = LabelEncoder().fit(classLabel)\n",
    "        labels = labelEncoder.transform(classLabel)\n",
    "        classes = list(labelEncoder.classes_)\n",
    "\n",
    "        return len(classes)\n",
    "\n",
    "\n",
    "    # reshaping the data to the number of bins sizes\n",
    "    def reshapeDataToBinSize(dataMatrix, numberOfFeatEachBin, bins):\n",
    "        ReshapedData = np.zeros((len(dataMatrix), numberOfFeatEachBin, bins))\n",
    "        start = 0\n",
    "        end = numberOfFeatEachBin\n",
    "\n",
    "        for i in range(1, bins + 1):\n",
    "            ReshapedData[:, :, i - 1] = dataMatrix[:, start:end * i]\n",
    "            start = end * i\n",
    "        return ReshapedData\n",
    "\n",
    "    # returns the TP, TN, FP and FN values\n",
    "    def getTPTNValues(test, testPred):\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        for i in range(len(testPred)):\n",
    "            if test[i] == testPred[i] == 1:\n",
    "                TP += 1\n",
    "            if testPred[i] == 1 and test[i] != testPred[i]:\n",
    "                FP += 1\n",
    "            if test[i] == testPred[i] == 0:\n",
    "                TN += 1\n",
    "            if testPred[i] == 0 and test[i] != testPred[i]:\n",
    "                FN += 1\n",
    "\n",
    "        return TP, TN, FP, FN\n",
    "    #resize data\n",
    "    def resizeData(table1, table2):\n",
    "        matrix1 = np.array(table1)\n",
    "        matrix2 = np.array(table2)\n",
    "    \n",
    "        matrix1Row, matrix1Col = matrix1.shape\n",
    "        matrix2Row, matrix2Col = matrix2.shape\n",
    "\n",
    "        sampleSize = matrix1Row if matrix1Row <= matrix2Row else matrix2Row\n",
    "\n",
    "        numSampleToSelect =sampleSize # (sampleSize * 95) / 100\n",
    "        reSizedMatrix1 = matrix1[np.random.choice(matrix1Row, numSampleToSelect, replace=False), :]\n",
    "        reSizedMatrix2 = matrix2[np.random.choice(matrix2Row, numSampleToSelect, replace=False), :]\n",
    "        \n",
    "        return np.vstack((reSizedMatrix1, reSizedMatrix2))\n",
    "\n",
    "    #function to RF, and Linear Regression, Decision Tree\n",
    "    # Function to remove target column and create a data frame from onlydef data_prep(df):\n",
    "    def data_prep(df):\n",
    "        feature_columns = df.columns[:-1]\n",
    "        df_features = pd.DataFrame(data=df, columns=feature_columns)\n",
    "        return df_features\n",
    "    #Standard Scaler\n",
    "    # Custom scaler function\n",
    "    def standardScaling(feature):\n",
    "        scaler = StandardScaler().fit(feature)\n",
    "        scaled_feature = scaler.transform(feature)\n",
    "        scaled_feat = pd.DataFrame(data = scaled_feature, columns =df_features.columns)\n",
    "        return scaled_feat\n",
    "    ########################################################################################################################\n",
    "\n",
    "    #load datasets\n",
    "    #path = 'datasets/G20'\n",
    "    path = './'\n",
    "    #pathResult=\"results/G20\"\n",
    "    pathResult=\"Results/\"+org+\"_sub_group/protein_subgroups\"\n",
    "    #ajout du nouveau fichier du dataset\n",
    "    #metaDataset = pd.read_csv(\"dataset.csv\",sep=\";\")\n",
    "    #metaDataset = pd.read_csv(\"dataset_gene.csv\",sep=\";\")s\n",
    "    print(metaDataset)\n",
    "    row,col = metaDataset.shape\n",
    "    count=1\n",
    "    for index in range(row) :\n",
    "        print(\"############################\"+metaDataset['featureGroup'][index]+\"############################################\")\n",
    "        print(str(count)+\" / \"+str(row))\n",
    "        #ajout de ton fichier du datastet\n",
    "        #META_DATA_FILE_NAME=\"G20 Gene Essential Paper.xlsx\"\n",
    "        META_DATA_FILE_NAME=excel_file\n",
    "        FEAT=str(metaDataset['code'][index])\n",
    "        FEAT_FILE=os.path.join(path, str(metaDataset['filename'][index]))\n",
    "        META_FILE=os.path.join(path, str(META_DATA_FILE_NAME))\n",
    "        RESULT_FILE=os.path.join(pathResult, str(FEAT)+\"_Result.json\")\n",
    "\n",
    "        print(FEAT_FILE+\"\\n\")\n",
    "        print(META_FILE+\"\\n\")\n",
    "\n",
    "        print(RESULT_FILE)\n",
    "        #open file \n",
    "        #FILE_SAVE=open(RESULT_FILE, 'a')\n",
    "        DICT_DL=dict()\n",
    "        DICT_DT=dict()\n",
    "        DICT_LR=dict()\n",
    "        DICT_RF=dict()\n",
    "\n",
    "        # Kegg data from G20 Paper\n",
    "        feature_data = pd.read_csv(FEAT_FILE)\n",
    "        print(feature_data)\n",
    "        genes = feature_data.index\n",
    "        #\n",
    "        meta_df = pd.read_excel(META_FILE, sheet_name=\"Sheet1\",engine=\"openpyxl\")\n",
    "        #nom du champ pour identifier les gene Gene_Locus\n",
    "        meta_idx = meta_df['Gene_Locus']\n",
    "        meta_idx = pd.Series([x.upper() for x in meta_idx.values])\n",
    "        meta_df = meta_df.set_index(keys=meta_idx)\n",
    "        print(meta_df['Gene_essentaility'])\n",
    "        #\n",
    "        # get class labels for dataset\n",
    "        #remplace class par Gene_essentaility\n",
    "        df_full = feature_data.merge(meta_df[['Gene_essentaility']], how='inner', left_index=True, right_index=True)\n",
    "        #Nucleotide feature\n",
    "        if 'X' in df_full.columns:\n",
    "            df_full.drop('X', axis = 1, inplace=True)\n",
    "\n",
    "        # class mappings, 1 = Essential and 0 = Non-Essential\n",
    "        #deux class NE et E \n",
    "        #mappings = {'Dispensable': 0, 'ExpectedEssential': 1, 'EdgeInsertionOnly': 0, 'Desulfovibrio-specific essential': 1, 'NotUnique': 0, 'OtherNoInsertion': 2}\n",
    "        mappings = {'NE': 0, 'E': 1}\n",
    "        classes = df_full.pop('Gene_essentaility')\n",
    "        essential_labels = classes.map(mappings)\n",
    "        df_full['essential'] = essential_labels\n",
    "\n",
    "        # remove unknowns\n",
    "        df_full = df_full[df_full['essential'] < 2]\n",
    "        #copy dataset\n",
    "        dataset_full=df_full.copy()\n",
    "        #\n",
    "        df_essential = df_full[df_full['essential'] == 1]\n",
    "        df_nonEssential = df_full[df_full['essential'] == 0]\n",
    "        # rebalance the classes\n",
    "        df_essential_oversample = pd.concat([df_essential, df_essential], ignore_index=True)\n",
    "        # sample non-essential genes\n",
    "        total_essential_samples = len(df_essential_oversample)\n",
    "        df_nonE_sample_RF = df_nonEssential.sample(2*total_essential_samples)\n",
    "        # combine essential and non-essential sets, drop gene name column\n",
    "        #balance data\n",
    "        #df_full = pd.concat([df_essential_oversample.iloc[:,:],df_nonE_sample_RF.iloc[:,:]], ignore_index=True)\n",
    "        #use unbalance data\n",
    "\n",
    "        df_full = pd.concat([df_essential.iloc[:,:],df_nonEssential.iloc[:,:]], ignore_index=True)\n",
    "\n",
    "        #df_full = pd.concat([df_essential_oversample.iloc[:,1:],df_nonE_sample_RF.iloc[:,1:]], ignore_index=True)\n",
    "        # To make original data as intact, deep copy\n",
    "        #df to RF\n",
    "        df_RF = df_full.copy()\n",
    "        #\n",
    "        print(df_full )\n",
    "        #\n",
    "\n",
    "        ###########RANDOM FOREST\n",
    "        #calling the function prepare to get de feature columns\n",
    "        df_features = data_prep(df_RF)\n",
    "        #spiting the data to train and test the model\n",
    "        X=df_features.copy()\n",
    "        y=df_RF['essential'].copy()\n",
    "        X_train,X_test, y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "        # Calling the scaler function by passing X_train and X_test to get the scaled data set\n",
    "        X_train_scaled = standardScaling(X_train)\n",
    "        X_test_scaled = standardScaling(X_test)\n",
    "        #\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        # Print X_train scaled data\n",
    "        row,col = X_train_scaled.shape\n",
    "        print(row)\n",
    "        print(X_train_scaled.head())\n",
    "        #\n",
    "        #####################RANDOM FOREST BUILD AND TRAIN\n",
    "\n",
    "        rf_model = RandomForestClassifier(n_estimators=200)\n",
    "        rf_model.fit(X_train_scaled, y_train)\n",
    "        #min_impurity_split=None,\n",
    "        RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, \n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
    "                    oob_score=False, random_state=None, verbose=0,\n",
    "                    warm_start=False)\n",
    "        #\n",
    "        # Prediction using Random Forest Model\n",
    "        rf_prediction = rf_model.predict(X_test_scaled)# Evaluations\n",
    "        train_probs = rf_model.predict_proba(X_train_scaled)[:,1]\n",
    "        probs = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        score_roc_auc=roc_auc_score(y_test, probs)\n",
    "        #\n",
    "        base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n",
    "        model_fpr, model_tpr, _ = roc_curve(y_test, probs)\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.rcParams['font.size'] = 16\n",
    "        plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "        plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "        plt.legend();\n",
    "        plt.xlabel('False Positive Rate');\n",
    "        plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "        figure_save=os.path.join(pathResult, str(FEAT)+\"roc_curve.png\")\n",
    "        plt.savefig(figure_save)\n",
    "        #plt.show();\n",
    "        print(\"ROC AUC\")\n",
    "        print(score_roc_auc)\n",
    "        print(train_probs)\n",
    "        print('Classification Report: \\n')\n",
    "        result=classification_report(y_test,rf_prediction,output_dict=True) #,output_dict=True\n",
    "        print(result)\n",
    "        print('\\nConfusion Matrix: \\n')\n",
    "        print(confusion_matrix(y_test,rf_prediction))\n",
    "        # display actual vs. predicted values\n",
    "        #%load_ext google.colab.data_table\n",
    "        rf_pred_table = pd.DataFrame({'Predicted': rf_prediction, 'Actual': y_test})\n",
    "        DICT_RF={\"FEAT\":FEAT,\"MODEL\":\"RF\",\"METRICS\":result,\"AUC\":score_roc_auc}\n",
    "\n",
    "        DATA_COLUMNS = df_features.columns.values\n",
    "        feature_columns = []\n",
    "        print(df_features)\n",
    "        for feature_name in DATA_COLUMNS:\n",
    "            feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                                dtype=tf.float32))\n",
    "        print(df_features)\n",
    "        #\n",
    "        feature_imp = pd.Series(rf_model.feature_importances_,index=feature_columns).sort_values(ascending=False)\n",
    "        FeatImp_FILE=os.path.join(pathResult, str(FEAT)+\"_FeatureImportance.csv\")\n",
    "        feature_imp.to_csv(FeatImp_FILE, index=True, sep=\";\")\n",
    "        print(\"############################Feature Importanace BEGIN#######################################\")\n",
    "        print(feature_imp)\n",
    "        #maptplot\n",
    "        sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "        # Add labels to your graph\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(\"Visualizing Important Features\")\n",
    "        plt.legend()\n",
    "        figure_save=os.path.join(pathResult, str(FEAT)+\"_FeatureImportance.png\")\n",
    "        plt.savefig(figure_save)\n",
    "        #plt.savefig(figure_save)\n",
    "        print(\"############################Feature Importanace END#######################################\")\n",
    "        with open(RESULT_FILE, 'w') as fp:\n",
    "            json.dump(DICT_RF, fp)\n",
    "\n",
    "        print(\"RANDOM FOREST RESULT \"+ str(rf_pred_table))\n",
    "        ###############################################################END RANDOM FOREST#####################################\n",
    "\n",
    "        print(\"############################END \"+metaDataset['featureGroup'][index]+\"############################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_org = sorted(os.listdir(\"subgroup_feat\"))\n",
    "#subgroup_org = ['dti_Feature_collection', 'tcm_Feature_collection', 'top_Feature_collection']\n",
    "print(subgroup_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_list = sorted(os.listdir(\"excel_epath\"))\n",
    "print(excel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for i in range(len(subgroup_org)):\n",
    "    print(\"copy of\",excel_list[i])\n",
    "    src_excel=r'excel_epath/'+excel_list[i]\n",
    "    dest_excel=r'./'+excel_list[i]\n",
    "    shutil.copyfile(src_excel, dest_excel)\n",
    "    #coipie des features pour les genes\n",
    "\n",
    "    print(\"####copy of features files#####\")\n",
    "    src_feats = r'subgroup_feat/'+subgroup_org[i]+'/subgroup_protein'\n",
    "    dest_feats = r'./'\n",
    "\n",
    "    files=os.listdir(src_feats)\n",
    "    for fname in files:\n",
    "        shutil.copy2(os.path.join(src_feats,fname), dest_feats)\n",
    "    x = subgroup_org[i].split(\"_\")\n",
    "    org = x[0]\n",
    "    root = \"Results\"\n",
    "    path = os.path.join(root, org+\"_sub_group\")\n",
    "    #os.mkdir(path)\n",
    "    ## creation des sous repertoire\n",
    "    #sub_path_gene = os.path.join(path, \"gene_subgroups\")\n",
    "    sub_path_protein = os.path.join(path, \"protein_subgroups\")\n",
    "    os.mkdir(sub_path_protein)\n",
    "    #sub_path_gene = os.path.join(path,protein_subgroups)\n",
    "    metaDataset = pd.read_csv(\"dataset.csv\",sep=\";\")\n",
    "    main_function(org,excel_list[i],metaDataset)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import csv\n",
    "import json \n",
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cette fonction section tous les fichiers au format csv qui correspondent\n",
    "aux feautures importance et le notoye afin d'avoir deux colone une pour\n",
    "le non du feaure et l'autre pour le score obtenu lors de l'execution du modele'''\n",
    "def clean_feat_importance(path):\n",
    "    #path = './'\n",
    "    file1 = find_csv_filenames(path)\n",
    "    #print(\"#############\",file1)\n",
    "    fold_name = path+\"/\"+\"feat_importance\"\n",
    "    !mkdir $fold_name\n",
    "    for i in range(len(file1)):\n",
    "        f = open(path+\"/\"+file1[i])\n",
    "        content = list(f)\n",
    "        feat_names =[]\n",
    "        scores = []\n",
    "        characters = \"'\"\n",
    "        for j in range(1,len(content)):\n",
    "            x = content[j].split(\",\")\n",
    "            feat = x[0].split(\"=\")\n",
    "            feature = ''.join( x for x in feat[1] if x not in characters)\n",
    "            y = x[5].split(\";\")\n",
    "            feat_names.append(feature)\n",
    "            scores.append(y[1])\n",
    "            headerList = ['Feature_name','score']\n",
    "            name_file = file1[i].split(\".\")\n",
    "        with open(fold_name+\"/\"+name_file[0]+\".csv\", 'w') as file: \n",
    "            dw = csv.DictWriter(file, delimiter=',', fieldnames=headerList) \n",
    "            dw.writeheader()\n",
    "        f.close()\n",
    "        f = open(fold_name+\"/\"+name_file[0]+\".csv\",\"a\", newline=\"\")\n",
    "        for k in range(len(content)-1):\n",
    "            tuple1 = (feat_names[k],scores[k])\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(tuple1)\n",
    "        f.close()\n",
    "        #conversion en fichier excel\n",
    "        ex_file = fold_name+\"/\"+name_file[0]+\".csv\"\n",
    "        read_file = pd.read_csv (ex_file)\n",
    "        read_file.to_excel (fold_name+\"/\"+name_file[0]+\".xlsx\", index = None, header=True)\n",
    "        #suppression de l'ancien fichier csv\n",
    "        !rm $ex_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = \"./Results1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sub_feat_importance(folder):\n",
    "    for parent_folder in os.listdir(folder):\n",
    "        print(\"parent:\",parent_folder)\n",
    "        for child_folder in os.listdir(folder+\"/\"+parent_folder):\n",
    "            path = folder+\"/\"+parent_folder+\"/\"+child_folder\n",
    "            print(path)\n",
    "            clean_feat_importance(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent: dpg_sub_group\n",
      "./Results1/dpg_sub_group/gene_subgroups\n",
      "mkdir: impossible de créer le répertoire «./Results1/dpg_sub_group/gene_subgroups/feat_importance»: Le fichier existe\n",
      "./Results1/dpg_sub_group/protein_subgroups\n",
      "parent: drm_sub_group\n",
      "./Results1/drm_sub_group/gene_subgroups\n",
      "./Results1/drm_sub_group/protein_subgroups\n",
      "parent: dpr_sub_group\n",
      "./Results1/dpr_sub_group/gene_subgroups\n",
      "./Results1/dpr_sub_group/protein_subgroups\n",
      "parent: dpi_sub_group\n",
      "./Results1/dpi_sub_group/gene_subgroups\n",
      "./Results1/dpi_sub_group/protein_subgroups\n"
     ]
    }
   ],
   "source": [
    "clean_sub_feat_importance(result_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerList = ['Feature_Code','Feature_Group','Feature_Sub_group','Precison','Recall','F1-score','AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_filenames( path_to_dir, suffix=\".json\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_code(json_file):\n",
    "    x = json_file.split(\"_R\")\n",
    "    y = x[0].split(\"/\")\n",
    "    return y[len(y)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(file):\n",
    "    fichier_json = open(file, 'r', encoding=\"utf-8\")   \n",
    "    with fichier_json as fichier:\n",
    "        data = json.load(fichier)      # load décode un fichier json\n",
    "        metrics = data['METRICS']\n",
    "        auc = data['AUC']\n",
    "        weighted_avg = metrics['weighted avg']\n",
    "        precision = weighted_avg['precision']\n",
    "        recall = weighted_avg['recall']\n",
    "        f1_score = weighted_avg['f1-score']\n",
    "    return {'precision':precision, 'recall':recall, 'f1-score':f1_score,  'auc':auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_values(path,files,name,feat_group): \n",
    "    for i in range(len(files)):\n",
    "        metrics = get_metrics(path+\"/\"+files[i])\n",
    "        f = open(name,\"a\", newline=\"\")\n",
    "        code = get_feature_code(path+\"/\"+files[i])\n",
    "        tuple1 = (code,feat_group,code,metrics['precision'],metrics['recall'],metrics['f1-score'],metrics['auc'])\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(tuple1)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parent_folder in os.listdir(result_folder):\n",
    "    #contruction du nom du fichier\n",
    "    x = parent_folder.split(\"_\")\n",
    "    name_org = x[0]+\"_featureGeneration.csv\"\n",
    "    name_file = x[0]+\"_featureGeneration.xlsx\"\n",
    "    #creation du fichier\n",
    "    path = result_folder+\"/\"+parent_folder\n",
    "    path_org = path+\"/\"+name_org\n",
    "    with open(path_org, 'w') as file: \n",
    "        dw = csv.DictWriter(file, delimiter=',', fieldnames=headerList) \n",
    "        dw.writeheader()\n",
    "    for child in os.listdir(path):\n",
    "        path_child = path+\"/\"+child\n",
    "        if os.path.isdir(path_child):\n",
    "            y = child.split(\"_\")\n",
    "            if y[0]== \"gene\":\n",
    "                group =\"Gene Sequence (GSF)\"\n",
    "            else:\n",
    "                group = \"Protein Sequence (PSF)\"\n",
    "            files = find_json_filenames(path_child)\n",
    "            write_values(path_child,files,path_org,group)\n",
    "    read_file = pd.read_csv(path_org)\n",
    "    read_file.to_excel (path+\"/\"+name_file, index = None, header=True)\n",
    "    !rm $path_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/billdjomkam/Bureau/Test_extract_features'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
